---
title: "Project for Reproducible Research"
author: "Nadim Muhammad, MichaÅ‚ Kulbat"
date: "26/05/2023"
format: html
editor: visual
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# 1. Importing libraries and loading dataset

(install Rtools43 first: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html)

```{r warning=FALSE}
#| warning: false
library(usethis)
library(devtools)
```

```{r warning=FALSE}
#| eval: false
devtools::install_github("dongyuanwu/RSBID")
```

```{r warning=FALSE}
#| warning: false
library(tictoc) #for checking the training time of models
library(corrplot) #visualization of correlation matrix
library(tidyr) #to tidy messy data
library(reshape2) #For creating correlation matrix for heatmap
library(ggplot2) #Tables, heatmaps, etc.
library(forecast) #forecast for simple models
library(dplyr) #data manipulation
library(magrittr) # %>% pipeline
library(formattable) #data manipulation
library(Metrics) #metrics
library(kableExtra) #complex tables, "kable" pipe syntax, helpfull in some cases
library(kernlab) #for SVM
library(MLmetrics) #metrics for machine learning (similar to Metrics library)
library(e1071) #svm
library(randomForest) #we need it for RandomForest
library(ranger) #we need it for RandomForest
library(xgboost) #xgboost model
library(caret) #precision metrics, knn
library(fastDummies) #dummy variables
library(RSBID) #We will need SMOTE_NC from it
library(tidyverse) #collection of packages for data science
```

Data loading:

```{r}
rain_df <- read.csv("weatherAUS.csv")
```

```{r}
tail(rain_df)
```

# 2. Exploration and preprocessing

## 2.1. RainTomorrow distribution

```{r}
# Check the original shape of the dataset (rows, columns)

dim(rain_df)
```


```{r}
# Check for missing data in target variable

sum(is.na(rain_df$RainTomorrow))
```

```{r}
# Drop the data not including target variable since using them may later negativley influence the model

rain_df <- rain_df %>% drop_na("RainTomorrow")
dim(rain_df)
```

```{r}
# Check distribution, look if balanced/imbalanced

table(rain_df$RainTomorrow)
```

```{r}
# Visualise distibution to check the impalance

barplot(table(rain_df$RainTomorrow)) 
```

There is a class imbalance in **RainTommorow** - Our target variable is imbalanced.

The data will be transformed from categorical into a numerical for process convinience where 1 will mean that it will rain tomorrow and 0 that it will not rain

```{r echo=TRUE, results='hide'}
# Replace objects Yes with 1 and No with 0

rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "Yes"] <- 1
rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "No"] <- 0
rain_df$RainTomorrow <- as.numeric(rain_df$RainTomorrow)
rain_df["RainTomorrow"]
```

## 2.2. Data tpyes

```{r}
# Check for data type and data amount in columns.

str(rain_df)
```

We can see that there are 23 variables in which:

**1 Date:** Date

**7 Integer:** Cloud9am, Cloud3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, WindGustSpeed.

**5 Character:** Location, WindGustDir, WindDir9am, WindDir3pm ,RainToday.

**9 Numerical:** Temp9am, Temp3pm, Pressure9am, Pressure3pm, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, RainTomorrow.  

```{r}
# Check for unique data for Categorical in order to decide weather one hot encoding is viable option.
for (i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(n_distinct(rain_df[,i]))
}
```

**Character** data have to be further transofrmed in order to be used in the model.

## 2.3. Search for invalid and missing data

```{r}
# Look for invalid data

summary(rain_df)
```

```{r}
# Look for missing data

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

There seems to be a lot of missing data epsepcially in: **Evaporation, Sunshine, Cloud9am, and Cloud3pm**

It seems that variables **Rainfall, Evaporation, WindGustSpeed, WindSpeed9am, WindSpeed3pm** have very high difference betwen the mean and max value we will investigate this data using box plot.

## 2.4. Handling missing data

IMPORTANT: We decided to limit our test and training data to those where 'Sunshine' variable is not null in order to reduce the amount of data used and to lower the number of null values in whole dataset

```{r}
rain_df <- rain_df %>% drop_na(c("Sunshine"))
```

```{r}
# Check for missing data

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

```{r}
# fill missing data for numeric attributes with mean and median (if there are outliers [max values are much bigger than mean values] in our attribute we will use median)

rain_df$Temp3pm[is.na(rain_df$Temp3pm)] <- mean(rain_df$Temp3pm, na.rm = TRUE)
rain_df$Temp9am[is.na(rain_df$Temp9am)] <- mean(rain_df$Temp9am, na.rm = TRUE)
rain_df$Cloud3pm[is.na(rain_df$Cloud3pm)] <- mean(rain_df$Cloud3pm, na.rm = TRUE)
rain_df$Cloud9am[is.na(rain_df$Cloud9am)] <- mean(rain_df$Cloud9am, na.rm = TRUE)
rain_df$Pressure3pm[is.na(rain_df$Pressure3pm)] <- mean(rain_df$Pressure3pm, na.rm = TRUE)
rain_df$Pressure9am[is.na(rain_df$Pressure9am)] <- mean(rain_df$Pressure9am, na.rm = TRUE)
rain_df$Humidity3pm[is.na(rain_df$Humidity3pm)] <- mean(rain_df$Humidity3pm, na.rm = TRUE)
rain_df$Humidity9am[is.na(rain_df$Humidity9am)] <- mean(rain_df$Humidity9am, na.rm = TRUE)
rain_df$WindSpeed3pm[is.na(rain_df$WindSpeed3pm)] <- median(rain_df$WindSpeed3pm, na.rm = TRUE)
rain_df$WindSpeed9am[is.na(rain_df$WindSpeed9am)] <- median(rain_df$WindSpeed9am, na.rm = TRUE)
rain_df$WindGustSpeed[is.na(rain_df$WindGustSpeed)] <- median(rain_df$WindGustSpeed, na.rm = TRUE)
rain_df$Sunshine[is.na(rain_df$Sunshine)] <- mean(rain_df$Sunshine, na.rm = TRUE)
rain_df$RainToday[is.na(rain_df$RainToday)] <- mode(rain_df$RainToday)
rain_df$Evaporation <- ifelse(is.na(rain_df$Evaporation), median(rain_df$Evaporation, na.rm = TRUE), rain_df$Evaporation)
rain_df$Rainfall <- ifelse(is.na(rain_df$Rainfall), median(rain_df$Rainfall, na.rm = TRUE), rain_df$Rainfall)
rain_df$MaxTemp <- ifelse(is.na(rain_df$MaxTemp), mean(rain_df$MaxTemp, na.rm = TRUE), rain_df$MaxTemp)
rain_df$MinTemp <- ifelse(is.na(rain_df$MinTemp), mean(rain_df$MinTemp, na.rm = TRUE), rain_df$MinTemp)
```

```{r}
# Look for missing data again

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

```{r}
# Check the amount of data after dropping sunshine variable na

dim(rain_df)
```

## 2.5. Handling the outliers

### Rainfall

```{r}
# Check for outliers using Box Plot

boxplot(Rainfall ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$Rainfall)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$Rainfall, 0.25)
Q3 <- quantile(rain_df$Rainfall, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$Rainfall > High)
count2 <- sum(rain_df$Rainfall < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# We decided to censor our data to value of 75, because our variable is not normally distributed (high density of data near 0)

rain_df$Rainfall[rain_df$Rainfall > 75] <- 75
```

```{r}
# Check changes on histogram

hist(rain_df$Rainfall)
```

```{r}
# Check Box Plot again

boxplot(Rainfall ~ RainTomorrow, data = rain_df)
```

### Evaporation

```{r}
# Check for outliers using Box Plot

boxplot(Evaporation ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$Evaporation)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$Evaporation, 0.25)
Q3 <- quantile(rain_df$Evaporation, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$Evaporation > High)
count2 <- sum(rain_df$Evaporation < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$Evaporation[rain_df$Evaporation > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$Evaporation)
```

```{r}
# Check Box Plot again

boxplot(Evaporation ~ RainTomorrow, data = rain_df)
```

### WindGustSpeed

```{r}
# Check for outliers using Box Plot

boxplot(WindGustSpeed ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindGustSpeed)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindGustSpeed, 0.25)
Q3 <- quantile(rain_df$WindGustSpeed, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindGustSpeed > High)
count2 <- sum(rain_df$WindGustSpeed < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindGustSpeed[rain_df$WindGustSpeed > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindGustSpeed)
```

```{r}
# Check Box Plot again

boxplot(WindGustSpeed ~ RainTomorrow, data = rain_df)
```

### WindSpeed9am


```{r}
# Check for outliers using Box Plot

boxplot(WindSpeed9am ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindSpeed9am)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindSpeed9am, 0.25)
Q3 <- quantile(rain_df$WindSpeed9am, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindSpeed9am > High)
count2 <- sum(rain_df$WindSpeed9am < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindSpeed9am[rain_df$WindSpeed9am > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindSpeed9am)
```

```{r}
# Check Box Plot again

boxplot(WindSpeed9am ~ RainTomorrow, data = rain_df)
```

### WindSpeed3pm

```{r}
# Check for outliers using Box Plot

boxplot(WindSpeed3pm ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindSpeed3pm)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindSpeed3pm, 0.25)
Q3 <- quantile(rain_df$WindSpeed3pm, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindSpeed3pm > High)
count2 <- sum(rain_df$WindSpeed3pm < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindSpeed3pm[rain_df$WindSpeed3pm > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindSpeed3pm)
```

```{r}
# Check Box Plot again

boxplot(WindSpeed3pm ~ RainTomorrow, data = rain_df)
```

There was a lot of outliers in variables **Rainfall, Evaporation, WindGustSpeed, WindSpeed9am, WindSpeed3pm** after further investigation we decided to change the outstanding values, using IQR in order to determine the maximal and minimal value we changed the data respectivley.

## 2.6. Data Simplification

```{r}
# Since the wind direction were wide spread and very specific we decided to simplify the data by puting them into categories based on dominant direction, filled the missing data for wind direction using mode.

rain_df$WindDir9am[rain_df$WindDir9am == "NNW"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NNE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "N"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "ENE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "ESE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "E"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SSE"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SSW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "s"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "WSW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "WNW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "NW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "W"] <- "W"

rain_df$WindDir9am[rain_df$WindDir9am == "NNW"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NNE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "N"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "ENE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "ESE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "E"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SSE"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SSW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "s"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "WSW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "WNW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "NW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "W"] <- "W"

rain_df$WindDir3pm[rain_df$WindDir3pm == "NNW"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NNE"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NE"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "N"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "ENE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "ESE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "E"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SSE"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SSW"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SW"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "S"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "WSW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "WNW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "W"] <- "W"

rain_df$WindGustDir[rain_df$WindGustDir == "NNW"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "NNE"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "NE"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "N"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "ENE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "ESE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "SE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "E"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "SSE"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "SSW"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "SW"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "S"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "WSW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "WNW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "NW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "W"] <- "W"

tail(rain_df)

#Filling NA with mode

Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

rain_df$WindDir9am[is.na(rain_df$WindDir9am)] <- Mode(rain_df$WindDir9am)
rain_df$WindDir3pm[is.na(rain_df$WindDir3pm)] <- Mode(rain_df$WindDir3pm)
rain_df$WindGustDir[is.na(rain_df$WindGustDir)] <- Mode(rain_df$WindGustDir)

```

## 2.7. Handling Date

```{r}
n_distinct(rain_df$Date)
```

```{r}
# Change into datetime format in order to extarct month.

rain_df$Date <- as.POSIXct(rain_df$Date)
rain_df$Date <- month(rain_df$Date)
names(rain_df)[names(rain_df) == "Date"] <- "Month"

tail(rain_df)

```

# 3. Selecting Features

## 3.1. Creating new numerical variables

```{r}
# Creating delta varibles to check if changes in relation to data for example: change of wind speed from 9am to 3pm has impact on 'raintommorow' prediction

rain_df$DeltaWindSpeed <- rain_df$WindSpeed3pm - rain_df$WindSpeed9am
rain_df$DeltaHumidity <- rain_df$Humidity3pm - rain_df$Humidity9am
rain_df$DeltaPressure <- rain_df$Pressure3pm - rain_df$Pressure9am
rain_df$DeltaCloud <- rain_df$Cloud3pm - rain_df$Cloud9am
```

```{r}
tail(rain_df)
```

```{r}
#Looking for correlation with the 'RainTommorow' to pick the best attributes

Num_rain_df <- data.frame(rain_df$Month, rain_df$MinTemp, rain_df$MaxTemp, rain_df$Rainfall, rain_df$Evaporation, rain_df$Sunshine, rain_df$WindGustSpeed, rain_df$WindSpeed9am, rain_df$WindSpeed3pm, rain_df$Humidity9am, rain_df$Humidity3pm, rain_df$Pressure9am, rain_df$Pressure3pm, rain_df$Cloud9am, rain_df$Cloud3pm, rain_df$Temp9am, rain_df$Temp3pm, rain_df$RainTomorrow, rain_df$DeltaWindSpeed, rain_df$DeltaHumidity, rain_df$DeltaPressure, rain_df$DeltaCloud) 

cor(Num_rain_df[, colnames(Num_rain_df) !="RainTomorrow"], Num_rain_df$rain_df.RainTomorrow)
```

Most correlated more than **0.09** with **RainTommorow**:

**Sunshine, Pressure9am, Pressure3pm, Temp3pm, MaxTemp, Evaporation, WindGustSpeed, Rainfall, Cloud9am, Humidity9am, DeltaHumidity, Cloud3pm, Humidity3pm, DeltaPressure.**

## 3.2. Creating new categorical variable

```{r}
# Like the previous case but with categorical data creating the variable to investigate relation of change in wind direction to the 'RainTommorow'

rain_df$WindChange <- paste(rain_df$WindDir3pm, rain_df$WindDir9am, sep="")

tail(rain_df$WindChange)
```

## 3.3. Investigating object-typed attribute distribution

#```{r}
pie()
```

#```{r}
pie(WindDir9am ~ RainTomorrow, data = rain_df)
```

```{r}
# Replace objects Yes with 1 and No with 0

rain_df["RainToday"][rain_df["RainToday"] == "Yes"] <- 1
rain_df["RainToday"][rain_df["RainToday"] == "No"] <- 0
rain_df$RainToday <- as.numeric(rain_df$RainToday)
rain_df$RainToday[is.na(rain_df$RainToday)] <- Mode(rain_df$RainToday)
```


## 3.4. Final feature selection

```{r}
#Select best features

rain_df_selected <- rain_df[,c("Month","RainTomorrow","Sunshine","Pressure9am","Pressure3pm","Temp3pm","MaxTemp","WindGustSpeed","Rainfall","Cloud9am","Humidity9am","DeltaHumidity","Cloud3pm","Evaporation","RainToday","Humidity3pm","DeltaPressure","WindGustDir","WindChange")]

tail(rain_df_selected)
```

```{r}
#Look for correlation between features

cor_df <- round(cor(rain_df_selected[,-c(18,19)]), 2)

m_cor_df <- melt(cor_df)

ggplot(data = m_cor_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + geom_text(aes(label = value), size = 1) + scale_fill_gradient2(low = "darkred", high = "red",)
```

Strong correlation:

**Pressure9am** and **Pressure3pm**

**MaxTemp** and **Temp3pm**

```{r}
#Drop features with strong correlation

rain_df_selected <- rain_df_selected[-4]
rain_df_selected <- rain_df_selected[-5]
```

```{r}
# Check selected data

tail(rain_df_selected)
```

```{r}
#Check again for strong correlation between features

cor_df <- round(cor(rain_df_selected[,-c(16,17)]), 2)

m_cor_df <- melt(cor_df)

ggplot(data = m_cor_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + geom_text(aes(label = value), size = 1) + scale_fill_gradient2(low = "darkred", high = "red",)
```

# 4. Balancing and Standarizing

First of all, we will be using SMOTE_NC (version of SMOTE which can use categorical variables) to rebalance our datasets:

```{r warning=FALSE}
#| output: false
categorical <- c("Month","RainTomorrow","RainToday","WindGustDir","WindChange")
rain_df_selected[,categorical] <- lapply(rain_df_selected[,categorical],factor)


data_selected<-SMOTE_NC(rain_df_selected,"RainTomorrow")
```

Now, we will create dummy variables for our nonbinary categorical variables (k-1 dummy variables for factors with k levels, this wasn't done in jupyter notebook code, but it should have been):

```{r warning=FALSE}
data_selected<-dummy_cols(data_selected, select_columns = c("Month","WindChange","WindGustDir"), remove_first_dummy = TRUE, remove_selected_columns = TRUE)
```

Now, we sould standarize our data. We will use min-max scaler for it:

```{r warning=FALSE}
process <- preProcess(data_selected, method=c("range"))

data_selected_normalized <- predict(process, data_selected)

for (i in 2:14)
{
  print(colnames(data_selected_normalized[i]))
  print(mean(data_selected_normalized[,i]))
}
```
Mean values are not the same as in the jupyter notebook Python code, most probably it is due to the fact, that SMOTE_NC function works in slightly different way - sill changes are a magnitude of $10^{-4}$.

# 5. Split train and test

```{r}
set.seed(111)

split1<- sample(nrow(data_selected_normalized),floor(nrow(data_selected_normalized)*0.7),replace=FALSE)

train <- data_selected_normalized[split1,]

test <- data_selected_normalized[-split1,]

head(train)

y_test=test[,1]
x_test=test[,-1]
```


# 6. Logistic Regression

First, let's count null accuracy in the data
```{r}
table(rain_df_selected[,2])

cat("Null accuracy in the data:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]))
```



```{r}

logistic_reg <- glm(RainTomorrow~.,family = binomial(link = "logit"),data=train)


predicted <- predict(logistic_reg, newdata = test, type = "response")


predicted<-round(predicted)

table(y_test,predicted)


cat("\n Accuracy of the model:",Accuracy(predicted,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted),"\n")
cat("Precision of the model",Precision(y_test,predicted),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(round(predict(logistic_reg, newdata = train, type = "response")),train[1]),"\n")
cat("Test set:",Accuracy(predicted,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")

```
We won't be doing hyperparameter tuning for logistic regression, as there is not much it could be done.

# 7. SVC
```{r}
SVM_model <- svm(RainTomorrow~., data=train)

predicted_SVM_model <- predict(SVM_model, newdata = test, type = "response")

predict_SVM_model_train <- predict(SVM_model, newdata = train, type="response")

head(predicted_SVM_model)

table(y_test,predicted_SVM_model)


cat("\n Accuracy of the model:",Accuracy(predicted_SVM_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_SVM_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_SVM_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_SVM_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predict_SVM_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_SVM_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")

```
We won't be doing CV and hyperparameter tuning for SVM model, due to very long training time (several hours, we couldn't find good package, which would work faster).


# 8. Random Forest


We will use Random Forest model with similar hyperparameters as in the Jupyter Notebook code (we don't have exact set of hyperparameters, so we will use what there is, also we will use smaller number of cross validation, because R is less optimal than python and the evaluation is much longer).
```{r}
RF_trControl <- trainControl(method = "cv", number=4,savePredictions="final")

RF_tuneGrid <- expand.grid(mtry=c(15,20,30,32),
                           splitrule=c("gini","extratrees"),
                           min.node.size=c(1,3))

tic()
RF_model <- train(RainTomorrow~.,
                  data=train,method="ranger",
                  trControl=RF_trControl, 
                  tuneGrid=RF_tuneGrid,
                  verbose=FALSE)
toc()

predicted_RF_model<-predict(RF_model,newdata = test, type="raw")

predicted_RF_model_train<-predict(RF_model,newdata = train, type="raw")

table(y_test,predicted_RF_model)

RF_model$results
RF_model$bestTune

cat("\n Accuracy of the model:",Accuracy(predicted_RF_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_RF_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_RF_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_RF_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predicted_RF_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_RF_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")
```
Overfitting in train data, or specification of this model.



# 9. xgboost

We chose similar hyperparameters as in jupyter notebook code. We did full grid search in our hyperparameter grid.
```{r warning=FALSE}
#| warning: false
XGB_trControl <- trainControl(method = "cv", number=4,savePredictions="final")

XGB_tuneGrid <- expand.grid(nrounds=c(150,200,225,250),
                           max_depth=c(6,7),
                           eta=c(0.15,0.2,0.225,0.25,0.3),
                           gamma=0,
                           colsample_bytree=c(0.8,0.75,0.7),
                           min_child_weight=1,
                           subsample=1)

tic()
XGB_model <- train(RainTomorrow~.,
                  data=train,method="xgbTree",
                  trControl=XGB_trControl, 
                  tuneGrid=XGB_tuneGrid)
toc()

print(XGB_model$bestTune)

predicted_XGB_model<-predict(XGB_model,newdata = test, type="raw")

predicted_XGB_model_train<-predict(XGB_model,newdata = train, type="raw")

table(y_test,predicted_XGB_model)

cat("\n Accuracy of the model:",Accuracy(predicted_XGB_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_XGB_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_XGB_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_XGB_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predicted_XGB_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_XGB_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")



```
Little bit of overfitting, but similar to what we have in jupyter notebook results.


# 10. Conclusion

MICHALS PART