---
title: "Project for Reproducible Research"
author: "Nadim Muhammad, Micha≈Ç Kulbat"
date: "26/05/2023"
format: html
editor: visual
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

## 1. Importing libraries and loading dataset

(install Rtools43 first: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html)

```{r}
library(devtools)
```

```{r}
#| eval: false
devtools::install_github("dongyuanwu/RSBID")
```

```{r}
#| warning: false
library(dplyr) #data manipulation
library(formattable) #data manipulation
library(Metrics) #metrics
library(MLmetrics) #metrics for machine learning (similar to Metrics library)
library(e1071) #svm
library(xgboost) #xgboost model
library(caret) #dummy variables, precision metrics, knn
library(RSBID) #We will need SMOTE_NC from it
library(tidyverse) #collection of packages for data science
```

Data loading:

```{r}
rain_df <- read.csv("weatherAUS.csv")
```

## 2. Exploration and preprocessing

```{r}
head(rain_df)
summary(rain_df)
sapply(rain_df, function(x) sum(is.na (x)))
```

```{r}
rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "Yes"] <- 1
rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "No"] <- 0
rain_df["RainTomorrow"]
```
MICHALS PART

## 3. Selecting Features

NADIMS + MICHALS PART

## 4. Balancing and Standarizing

NADIMS PART

## 5. Split train and test

```{r}
set.seed(111)

split1<- sample(nrow(rain_df),floor(nrow(rain_df)*0.7),replace=FALSE)

train <- rain_df[split1,]

test <- rain_df[-split1,]
head(train)
```

## 6. Logistic Regression
```{r}
logistic_reg <- glm(RainTomorrow~.,family = binomial(link = "logit"),data=train)
```
We won't be doing hyperparameter tuning for logistic regression, as there is not much it could be done.

## 7. SVC
```{r}
SVM_model <- svm(RainTomorrow~., data=train)

SVM_tuned <- tune.svm(RainTomorrow~., data=train,
                      degree = c(3,4),
                      gamma = c(0.001 ,.005 ,0.01 ,0.1),
                      cost = seq(0.1,1, by = 0.1),
                      epsilon = c(0.1,0.05)
                      )
print(SVM_tuned$best.performance)
SVM_tuned$best.model
```
## 8. Random Forest

NADIMS PART

## 9. xgboost

NADIMS PART

## 10. Conclusion

MICHALS PART
