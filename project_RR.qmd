---
title: "Project for Reproducible Research"
author: "Nadim Muhammad, MichaÅ‚ Kulbat"
date: "26/05/2023"
format: html
editor: visual
output: 
  html_document:
    theme: spacelab
    highlight: tango
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: true
---

# 1. Importing libraries and loading dataset

(install Rtools43 first: https://cran.r-project.org/bin/windows/Rtools/rtools43/rtools.html)

```{r warning=FALSE}
#| warning: false
library(usethis)
library(devtools)
```

```{r warning=FALSE}
#| eval: false
devtools::install_github("dongyuanwu/RSBID")
```

```{r warning=FALSE}
#| warning: false
library(caret) #precision metrics, knn
library(tictoc) #for checking the training time of models
library(corrplot) #visualization of correlation matrix
library(tidyr) #to tidy messy data
library(reshape2) #For creating correlation matrix for heatmap
library(ggplot2) #Tables, heatmaps, etc.
library(forecast) #forecast for simple models
library(dplyr) #data manipulation
library(magrittr) # %>% pipeline
library(formattable) #data manipulation
library(Metrics) #metrics
library(kableExtra) #complex tables, "kable" pipe syntax, helpfull in some cases
library(kernlab) #for SVM
library(MLmetrics) #metrics for machine learning (similar to Metrics library)
library(e1071) #svm
library(randomForest) #we need it for RandomForest
library(ranger) #we need it for RandomForest
library(xgboost) #xgboost model
library(fastDummies) #dummy variables
library(RSBID) #We will need SMOTE_NC from it
library(tidyverse) #collection of packages for data science
library(RSNNS) #neural network
```

Data loading:

```{r}
rain_df <- read.csv("weatherAUS.csv")
```

```{r}
tail(rain_df)
```

# 2. Exploration and preprocessing

## 2.1. RainTomorrow distribution

```{r}
# Check the original shape of the dataset (rows, columns)

dim(rain_df)
```

```{r}
# Check for missing data in target variable

sum(is.na(rain_df$RainTomorrow))
```

```{r}
# Drop the data not including target variable since using them may later negativley influence the model

rain_df <- rain_df %>% drop_na("RainTomorrow")
dim(rain_df)
```

```{r}
# Check distribution, look if balanced/imbalanced

table(rain_df$RainTomorrow)
```

```{r}
# Visualise distibution to check the impalance

barplot(table(rain_df$RainTomorrow)) 
```

There is a class imbalance in **RainTommorow** - Our target variable is imbalanced.

The data will be transformed from categorical into a numerical for process convinience where 1 will mean that it will rain tomorrow and 0 that it will not rain

```{r echo=TRUE, results='hide'}
# Replace objects Yes with 1 and No with 0

rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "Yes"] <- 1
rain_df["RainTomorrow"][rain_df["RainTomorrow"] == "No"] <- 0
rain_df$RainTomorrow <- as.numeric(rain_df$RainTomorrow)
rain_df["RainTomorrow"]
```

## 2.2. Data tpyes

```{r}
# Check for data type and data amount in columns.

str(rain_df)
```

We can see that there are 23 variables in which:

**1 Date:** Date

**7 Integer:** Cloud9am, Cloud3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, WindGustSpeed.

**5 Character:** Location, WindGustDir, WindDir9am, WindDir3pm ,RainToday.

**9 Numerical:** Temp9am, Temp3pm, Pressure9am, Pressure3pm, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, RainTomorrow.

```{r}
# Check for unique data for Categorical in order to decide weather one hot encoding is viable option.
for (i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(n_distinct(rain_df[,i]))
}
```

**Character** data have to be further transofrmed in order to be used in the model.

## 2.3. Search for invalid and missing data

```{r}
# Look for invalid data

summary(rain_df)
```

```{r}
# Look for missing data

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

There seems to be a lot of missing data epsepcially in: **Evaporation, Sunshine, Cloud9am, and Cloud3pm**

It seems that variables **Rainfall, Evaporation, WindGustSpeed, WindSpeed9am, WindSpeed3pm** have very high difference betwen the mean and max value we will investigate this data using box plot.

## 2.4. Handling missing data

IMPORTANT: We decided to limit our test and training data to those where 'Sunshine' variable is not null in order to reduce the amount of data used and to lower the number of null values in whole dataset

```{r}
rain_df <- rain_df %>% drop_na(c("Sunshine"))
```

```{r}
# Check for missing data

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

```{r}
# fill missing data for numeric attributes with mean and median (if there are outliers [max values are much bigger than mean values] in our attribute we will use median)

rain_df$Temp3pm[is.na(rain_df$Temp3pm)] <- mean(rain_df$Temp3pm, na.rm = TRUE)
rain_df$Temp9am[is.na(rain_df$Temp9am)] <- mean(rain_df$Temp9am, na.rm = TRUE)
rain_df$Cloud3pm[is.na(rain_df$Cloud3pm)] <- mean(rain_df$Cloud3pm, na.rm = TRUE)
rain_df$Cloud9am[is.na(rain_df$Cloud9am)] <- mean(rain_df$Cloud9am, na.rm = TRUE)
rain_df$Pressure3pm[is.na(rain_df$Pressure3pm)] <- mean(rain_df$Pressure3pm, na.rm = TRUE)
rain_df$Pressure9am[is.na(rain_df$Pressure9am)] <- mean(rain_df$Pressure9am, na.rm = TRUE)
rain_df$Humidity3pm[is.na(rain_df$Humidity3pm)] <- mean(rain_df$Humidity3pm, na.rm = TRUE)
rain_df$Humidity9am[is.na(rain_df$Humidity9am)] <- mean(rain_df$Humidity9am, na.rm = TRUE)
rain_df$WindSpeed3pm[is.na(rain_df$WindSpeed3pm)] <- median(rain_df$WindSpeed3pm, na.rm = TRUE)
rain_df$WindSpeed9am[is.na(rain_df$WindSpeed9am)] <- median(rain_df$WindSpeed9am, na.rm = TRUE)
rain_df$WindGustSpeed[is.na(rain_df$WindGustSpeed)] <- median(rain_df$WindGustSpeed, na.rm = TRUE)
rain_df$Sunshine[is.na(rain_df$Sunshine)] <- mean(rain_df$Sunshine, na.rm = TRUE)
rain_df$RainToday[is.na(rain_df$RainToday)] <- mode(rain_df$RainToday)
rain_df$Evaporation <- ifelse(is.na(rain_df$Evaporation), median(rain_df$Evaporation, na.rm = TRUE), rain_df$Evaporation)
rain_df$Rainfall <- ifelse(is.na(rain_df$Rainfall), median(rain_df$Rainfall, na.rm = TRUE), rain_df$Rainfall)
rain_df$MaxTemp <- ifelse(is.na(rain_df$MaxTemp), mean(rain_df$MaxTemp, na.rm = TRUE), rain_df$MaxTemp)
rain_df$MinTemp <- ifelse(is.na(rain_df$MinTemp), mean(rain_df$MinTemp, na.rm = TRUE), rain_df$MinTemp)
```

```{r}
# Look for missing data again

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

```{r}
# Check the amount of data after dropping sunshine variable na

dim(rain_df)
```

## 2.5. Handling the outliers

### Rainfall

```{r}
# Check for outliers using Box Plot

boxplot(Rainfall ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$Rainfall)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$Rainfall, 0.25)
Q3 <- quantile(rain_df$Rainfall, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$Rainfall > High)
count2 <- sum(rain_df$Rainfall < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# We decided to censor our data to value of 75, because our variable is not normally distributed (high density of data near 0)

rain_df$Rainfall[rain_df$Rainfall > 75] <- 75
```

```{r}
# Check changes on histogram

hist(rain_df$Rainfall)
```

```{r}
# Check Box Plot again

boxplot(Rainfall ~ RainTomorrow, data = rain_df)
```

### Evaporation

```{r}
# Check for outliers using Box Plot

boxplot(Evaporation ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$Evaporation)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$Evaporation, 0.25)
Q3 <- quantile(rain_df$Evaporation, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$Evaporation > High)
count2 <- sum(rain_df$Evaporation < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$Evaporation[rain_df$Evaporation > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$Evaporation)
```

```{r}
# Check Box Plot again

boxplot(Evaporation ~ RainTomorrow, data = rain_df)
```

### WindGustSpeed

```{r}
# Check for outliers using Box Plot

boxplot(WindGustSpeed ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindGustSpeed)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindGustSpeed, 0.25)
Q3 <- quantile(rain_df$WindGustSpeed, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindGustSpeed > High)
count2 <- sum(rain_df$WindGustSpeed < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindGustSpeed[rain_df$WindGustSpeed > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindGustSpeed)
```

```{r}
# Check Box Plot again

boxplot(WindGustSpeed ~ RainTomorrow, data = rain_df)
```

### WindSpeed9am

```{r}
# Check for outliers using Box Plot

boxplot(WindSpeed9am ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindSpeed9am)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindSpeed9am, 0.25)
Q3 <- quantile(rain_df$WindSpeed9am, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindSpeed9am > High)
count2 <- sum(rain_df$WindSpeed9am < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindSpeed9am[rain_df$WindSpeed9am > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindSpeed9am)
```

```{r}
# Check Box Plot again

boxplot(WindSpeed9am ~ RainTomorrow, data = rain_df)
```

### WindSpeed3pm

```{r}
# Check for outliers using Box Plot

boxplot(WindSpeed3pm ~ RainTomorrow, data = rain_df) 
```

```{r}
# Check on histogram

hist(rain_df$WindSpeed3pm)
```

```{r}
# Use IQR to define range of valid data 

Q1 <- quantile(rain_df$WindSpeed3pm, 0.25)
Q3 <- quantile(rain_df$WindSpeed3pm, 0.75)

IQR <- Q3 - Q1

Low <- Q1 - IQR * 1.5
High <- Q3 + IQR * 1.5

print(Low)
print(High)
```

```{r}
# Check the amount of outliers 

count1 <- sum(rain_df$WindSpeed3pm > High)
count2 <- sum(rain_df$WindSpeed3pm < Low)
count3 <- count1 + count2

print(count1)
print(count2)
```

```{r}
# Change outliers respectivley to upper boundry since (relativley) there is not too many outliers

rain_df$WindSpeed3pm[rain_df$WindSpeed3pm > High] <- High
```

```{r}
# Check changes on histogram

hist(rain_df$WindSpeed3pm)
```

```{r}
# Check Box Plot again

boxplot(WindSpeed3pm ~ RainTomorrow, data = rain_df)
```

There was a lot of outliers in variables **Rainfall, Evaporation, WindGustSpeed, WindSpeed9am, WindSpeed3pm** after further investigation we decided to change the outstanding values, using IQR in order to determine the maximal and minimal value we changed the data respectivley.

## 2.6. Data Simplification

```{r}
# Since the wind direction were wide spread and very specific we decided to simplify the data by puting them into categories based on dominant direction, filled the missing data for wind direction using mode.

rain_df$WindDir9am[rain_df$WindDir9am == "NNW"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NNE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "N"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "ENE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "ESE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "E"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SSE"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SSW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "s"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "WSW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "WNW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "NW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "W"] <- "W"

rain_df$WindDir9am[rain_df$WindDir9am == "NNW"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NNE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "NE"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "N"] <- "N"
rain_df$WindDir9am[rain_df$WindDir9am == "ENE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "ESE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SE"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "E"] <- "E"
rain_df$WindDir9am[rain_df$WindDir9am == "SSE"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SSW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "SW"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "s"] <- "S"
rain_df$WindDir9am[rain_df$WindDir9am == "WSW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "WNW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "NW"] <- "W"
rain_df$WindDir9am[rain_df$WindDir9am == "W"] <- "W"

rain_df$WindDir3pm[rain_df$WindDir3pm == "NNW"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NNE"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NE"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "N"] <- "N"
rain_df$WindDir3pm[rain_df$WindDir3pm == "ENE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "ESE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SE"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "E"] <- "E"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SSE"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SSW"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "SW"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "S"] <- "S"
rain_df$WindDir3pm[rain_df$WindDir3pm == "WSW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "WNW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "NW"] <- "W"
rain_df$WindDir3pm[rain_df$WindDir3pm == "W"] <- "W"

rain_df$WindGustDir[rain_df$WindGustDir == "NNW"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "NNE"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "NE"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "N"] <- "N"
rain_df$WindGustDir[rain_df$WindGustDir == "ENE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "ESE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "SE"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "E"] <- "E"
rain_df$WindGustDir[rain_df$WindGustDir == "SSE"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "SSW"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "SW"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "S"] <- "S"
rain_df$WindGustDir[rain_df$WindGustDir == "WSW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "WNW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "NW"] <- "W"
rain_df$WindGustDir[rain_df$WindGustDir == "W"] <- "W"

tail(rain_df)

#Filling NA with mode

Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}

rain_df$WindDir9am[is.na(rain_df$WindDir9am)] <- Mode(rain_df$WindDir9am)
rain_df$WindDir3pm[is.na(rain_df$WindDir3pm)] <- Mode(rain_df$WindDir3pm)
rain_df$WindGustDir[is.na(rain_df$WindGustDir)] <- Mode(rain_df$WindGustDir)

```

## 2.7. Handling Date

```{r}
n_distinct(rain_df$Date)
```

```{r}
# Change into datetime format in order to extarct month.

rain_df$Date <- as.POSIXct(rain_df$Date)
rain_df$Date <- month(rain_df$Date)
names(rain_df)[names(rain_df) == "Date"] <- "Month"

tail(rain_df)

```

# 3. Selecting Features

## 3.1. Creating new numerical variables

```{r}
# Creating delta varibles to check if changes in relation to data for example: change of wind speed from 9am to 3pm has impact on 'raintommorow' prediction

rain_df$DeltaWindSpeed <- rain_df$WindSpeed3pm - rain_df$WindSpeed9am
rain_df$DeltaHumidity <- rain_df$Humidity3pm - rain_df$Humidity9am
rain_df$DeltaPressure <- rain_df$Pressure3pm - rain_df$Pressure9am
rain_df$DeltaCloud <- rain_df$Cloud3pm - rain_df$Cloud9am
```

```{r}
tail(rain_df)
```

```{r}
#Looking for correlation with the 'RainTommorow' to pick the best attributes

Num_rain_df <- data.frame(rain_df$Month, rain_df$MinTemp, rain_df$MaxTemp, rain_df$Rainfall, rain_df$Evaporation, rain_df$Sunshine, rain_df$WindGustSpeed, rain_df$WindSpeed9am, rain_df$WindSpeed3pm, rain_df$Humidity9am, rain_df$Humidity3pm, rain_df$Pressure9am, rain_df$Pressure3pm, rain_df$Cloud9am, rain_df$Cloud3pm, rain_df$Temp9am, rain_df$Temp3pm, rain_df$RainTomorrow, rain_df$DeltaWindSpeed, rain_df$DeltaHumidity, rain_df$DeltaPressure, rain_df$DeltaCloud) 

cor(Num_rain_df[, colnames(Num_rain_df) !="RainTomorrow"], Num_rain_df$rain_df.RainTomorrow)
```

Most correlated more than **0.09** with **RainTommorow**:

**Sunshine, Pressure9am, Pressure3pm, Temp3pm, MaxTemp, Evaporation, WindGustSpeed, Rainfall, Cloud9am, Humidity9am, DeltaHumidity, Cloud3pm, Humidity3pm, DeltaPressure.**

## 3.2. Creating new character variable

```{r}
# Like the previous case but with categorical data creating the variable to investigate relation of change in wind direction to the 'RainTommorow'

rain_df$WindChange <- paste(rain_df$WindDir3pm, rain_df$WindDir9am, sep="")

tail(rain_df$WindChange)
```

## 3.3. Investigating object-typed attribute distribution

We had to move this line of code here in order to avoid plot error where RainToday variable was showing a "character"

```{r warning=FALSE}
# Replace objects Yes with 1 and No with 0
rain_df["RainToday"][rain_df["RainToday"] == "Yes"] <- 1
rain_df["RainToday"][rain_df["RainToday"] == "No"] <- 0
rain_df$RainToday <- as.numeric(rain_df$RainToday)
rain_df$RainToday[is.na(rain_df$RainToday)] <- Mode(rain_df$RainToday)
```

We had to add data filtration since we cannot plot it the same way as previous boxplots.

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

fdf1 <- subset(rain_df, RainTomorrow == 1)
fdf0 <- subset(rain_df, RainTomorrow == 0)

pie(table(fdf1$WindDir9am), main = "RainRomorrow 1")
pie(table(fdf0$WindDir9am), main = "RainRomorrow 0") 
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

pie(table(fdf1$WindDir3pm), main = "RainRomorrow 1")
pie(table(fdf0$WindDir3pm), main = "RainRomorrow 0") 
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

pie(table(fdf1$WindGustDir), main = "RainRomorrow 1")
pie(table(fdf0$WindGustDir), main = "RainRomorrow 0") 
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

pie(table(fdf1$WindChange), main = "RainRomorrow 1")
pie(table(fdf0$WindChange), main = "RainRomorrow 0")
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

barplot(prop.table(table(fdf1$WindChange)),main = "RainRomorrow 1")
barplot(prop.table(table(fdf0$WindChange)),main = "RainRomorrow 0")
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

barplot(prop.table(table(fdf1$Location)),main = "RainRomorrow 1")
barplot(prop.table(table(fdf0$Location)),main = "RainRomorrow 0")
```

```{r}
# Visualise and look for noticable between RainTomorrow 1 and 0

pie(table(fdf1$RainToday), main = "RainRomorrow 1")
pie(table(fdf0$RainToday), main = "RainRomorrow 0")
```

Looking at distributions above we decided to use **RainToday**, **WindChange** and **WindGusDir**, because of the difference in plots between situations where **RainTommorow** is equal to 0 and 1. We decided not to include Location because of not that big difference in relation to number of categories in that variable. We also did not include **WindDir9am** and **WindDir3pm**, because of big possibility of correlation to already used **WindGusDir**.

```{r}
# Look for missing data again

for(i in 1:dim(rain_df)[2])
{
  print(colnames(rain_df[i]))
  print(sum(is.na(rain_df[,i])))
}
```

## 3.4. Final feature selection

```{r}
#Select best features

rain_df_selected <- rain_df[,c("Month","RainTomorrow","Sunshine","Pressure9am","Pressure3pm","Temp3pm","MaxTemp","WindGustSpeed","Rainfall","Cloud9am","Humidity9am","DeltaHumidity","Cloud3pm","Evaporation","RainToday","Humidity3pm","DeltaPressure","WindGustDir","WindChange")]

tail(rain_df_selected)
```

```{r}
#Look for correlation between features

cor_df <- round(cor(rain_df_selected[,-c(18,19)]), 2)

m_cor_df <- melt(cor_df)

ggplot(data = m_cor_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + geom_text(aes(label = value), size = 1) + scale_fill_gradient2(low = "darkred", high = "red",)
```

Strong correlation:

**Pressure9am** and **Pressure3pm**

**MaxTemp** and **Temp3pm**

```{r}
#Drop features with strong correlation

rain_df_selected <- rain_df_selected[-4]
rain_df_selected <- rain_df_selected[-5]
```

```{r}
# Check selected data

tail(rain_df_selected)
```

```{r}
#Check again for strong correlation between features

cor_df <- round(cor(rain_df_selected[,-c(16,17)]), 2)

m_cor_df <- melt(cor_df)

ggplot(data = m_cor_df, aes(x=Var1, y=Var2, fill=value)) + geom_tile() + geom_text(aes(label = value), size = 1) + scale_fill_gradient2(low = "darkred", high = "red",)
```

# 4. Balancing and Standarizing

First of all, we will be using SMOTE_NC (version of SMOTE which can use categorical variables) to rebalance our datasets:

```{r warning=FALSE}
#| output: false
categorical <- c("Month","RainTomorrow","RainToday","WindGustDir","WindChange")
rain_df_selected[,categorical] <- lapply(rain_df_selected[,categorical],factor)


data_selected<-SMOTE_NC(rain_df_selected,"RainTomorrow")
```

Now, we will create dummy variables for our nonbinary categorical variables (k-1 dummy variables for factors with k levels, this wasn't done in jupyter notebook code, but it should have been):

```{r warning=FALSE}
data_selected<-dummy_cols(data_selected, select_columns = c("Month","WindChange","WindGustDir"), remove_first_dummy = TRUE, remove_selected_columns = TRUE)
```

Now, we sould standarize our data. We will use min-max scaler for it:

```{r warning=FALSE}
process <- preProcess(data_selected, method=c("range"))

data_selected_normalized <- predict(process, data_selected)

for (i in 2:14)
{
  print(colnames(data_selected_normalized[i]))
  print(mean(data_selected_normalized[,i]))
}
```

Mean values are not the same as in the jupyter notebook Python code, most probably it is due to the fact, that SMOTE_NC function works in slightly different way - sill changes are a magnitude of $10^{-4}$.

# 5. Split train and test

```{r}
set.seed(111)

split1<- sample(nrow(data_selected_normalized),floor(nrow(data_selected_normalized)*0.7),replace=FALSE)

train <- data_selected_normalized[split1,]

test <- data_selected_normalized[-split1,]

head(train)

y_test=test[,1]
x_test=test[,-1]
```

# 6. Logistic Regression

First, let's count null accuracy in the data

```{r}
table(rain_df_selected[,2])

cat("Null accuracy in the data:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]))
```

```{r}

logistic_reg <- glm(RainTomorrow~.,family = binomial(link = "logit"),data=train)


predicted <- predict(logistic_reg, newdata = test, type = "response")


predicted<-round(predicted)

table(y_test,predicted)


cat("\n Accuracy of the model:",Accuracy(predicted,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted),"\n")
cat("Precision of the model",Precision(y_test,predicted),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(round(predict(logistic_reg, newdata = train, type = "response")),train[1]),"\n")
cat("Test set:",Accuracy(predicted,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")

```

We won't be doing hyperparameter tuning for logistic regression, as there is not much it could be done.

# 7. SVC

```{r}
SVM_model <- svm(RainTomorrow~., data=train)

predicted_SVM_model <- predict(SVM_model, newdata = test, type = "response")

predict_SVM_model_train <- predict(SVM_model, newdata = train, type="response")

head(predicted_SVM_model)

table(y_test,predicted_SVM_model)


cat("\n Accuracy of the model:",Accuracy(predicted_SVM_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_SVM_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_SVM_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_SVM_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predict_SVM_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_SVM_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")

```

We won't be doing CV and hyperparameter tuning for SVM model, due to very long training time (several hours, we couldn't find good package, which would work faster).

# 8. Random Forest

We will use Random Forest model with similar hyperparameters as in the Jupyter Notebook code (we don't have exact set of hyperparameters, so we will use what there is, also we will use smaller number of cross validation, because R is less optimal than python and the evaluation is much longer).

```{r}
RF_trControl <- trainControl(method = "cv", number=4,savePredictions="final")

RF_tuneGrid <- expand.grid(mtry=c(15,20,30,32),
                           splitrule=c("gini","extratrees"),
                           min.node.size=c(1,3))

tic()
RF_model <- caret::train(RainTomorrow~.,
                  data=train,method="ranger",
                  trControl=RF_trControl, 
                  tuneGrid=RF_tuneGrid,
                  verbose=FALSE)
toc()

predicted_RF_model<-predict(RF_model,newdata = test, type="raw")

predicted_RF_model_train<-predict(RF_model,newdata = train, type="raw")

table(y_test,predicted_RF_model)

RF_model$results
RF_model$bestTune

cat("\n Accuracy of the model:",Accuracy(predicted_RF_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_RF_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_RF_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_RF_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predicted_RF_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_RF_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")
```

Overfitting in train data, or specification of this model.

# 9. xgboost

We chose similar hyperparameters as in jupyter notebook code. We did full grid search in our hyperparameter grid.

```{r warning=FALSE}
#| warning: false
XGB_trControl <- trainControl(method = "cv", number=4,savePredictions="final")

XGB_tuneGrid <- expand.grid(nrounds=c(150,200,225,250),
                           max_depth=c(6,7),
                           eta=c(0.15,0.2,0.225,0.25,0.3),
                           gamma=0,
                           colsample_bytree=c(0.8,0.75,0.7),
                           min_child_weight=1,
                           subsample=1)

tic()
XGB_model <- caret::train(RainTomorrow~.,
                  data=train,method="xgbTree",
                  trControl=XGB_trControl, 
                  tuneGrid=XGB_tuneGrid,
                  verbose=FALSE,
                  verbosity=0)
toc()
```

```{r warning=FALSE}
#| warning: false
print(XGB_model$bestTune)

predicted_XGB_model<-predict(XGB_model,newdata = test, type="raw")

predicted_XGB_model_train<-predict(XGB_model,newdata = train, type="raw")

table(y_test,predicted_XGB_model)

cat("\n Accuracy of the model:",Accuracy(predicted_XGB_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_XGB_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_XGB_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_XGB_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predicted_XGB_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_XGB_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")



```

Little bit of overfitting, but similar to what we have in jupyter notebook results.

# 10. Simple Neural Network

```{r warning=FALSE}
#| warning: false
#| eval: false
NN_trControl <- trainControl(method = "cv", number=4,savePredictions="final")

NN_tuneGrid <- expand.grid(layer1=c(20,50,100),
                           layer2=c(10,50,100,150),
                           layer3=c(5,20,40,80),
                           decay=c(0.1,0,2))
tic()
NN_model <- caret::train(RainTomorrow~.,
                  data=train,method="mlpWeightDecayML",
                  trControl=NN_trControl, 
                  tuneGrid=NN_tuneGrid)
toc()

print(NN_model$bestTune)
NN_model
predicted_NN_model<-predict(NN_model,newdata = test, type="raw")

predicted_NN_model_train<-predict(NN_model,newdata = train, type="raw")

table(y_test,predicted_NN_model)

cat("\n Accuracy of the model:",Accuracy(predicted_NN_model,y_test),"\n")
cat("Recall of the model:",Recall(y_test,predicted_NN_model),"\n")
cat("Precision of the model",Precision(y_test,predicted_NN_model),"\n")
cat("ROC AUC of the model:",AUC(y_test,predicted_NN_model),"\n\n")

cat("Accuracy scores in training set, test set and null accuracy: \n")
cat("Training set:",Accuracy(predicted_NN_model_train,train[,1]),"\n")
cat("Test set:",Accuracy(predicted_NN_model,y_test),"\n")
cat("Null:", table(rain_df_selected[,2])[1]/(table(rain_df_selected[,2])[1]+table(rain_df_selected[,2])[2]),"\n")

#NOT WORKING, ALL PREDICTIONS ARE SET TO 0. DONT RUN IT


```

# 11. General conclusion on reproduction of ML model

## Exploration

After uploading the dataset we retraced our steps deleting rows with missing data in target variable RainTomorrow and checked the data types with only difference being the data types naming since in R there is a distinction between intenger and numerical data, there is **1 Date**: Date. **5 Character**: Location, WindGustDir, WindDir9am, WindDir3pm, RainToday. **9 Numerical**: Temp9am, Temp3pm, Pressure9am, Pressure3pm, MinTemp, MaxTemp, Rainfall, Evaporation, Sunshine, RainTomorrow. **7 Integer**: Cloud9am, Cloud3pm, WindSpeed9am, WindSpeed3pm, Humidity9am, Humidity3pm, WindGustSpeed.

## Preprocessing

We checked for missing data using a sum(is.na()) in a loop and invalid data by analysing the results from summary() we decided dropped the missing data rows for Sunshine since as before there was a large chunk of missing data in it, the left missng data has been filled as in original case using mean and median respectfully depending on any possibility of invalid data noticed in analysis. The Suspicious numerical data remained the same: **Rainfall**, **Evaporation**, **WindGustSpeed**, **WindSpeed9am**, **WindSpeed3pm** and had very high difference betwen the mean and max value we used boxplot() to look for outliers as in python case. Afte analyzing the results with the outliers in data we recreated lower and upper boundries using interquartile range for **Evaporation**, **WindGustSpeed**, **WindSpeed9am**, **WindSpeed3pm**, in the **Rainfall** we recreated the boundry of 75 since the results above this were pointing towards 1 in **RainTomorrow**. The changes between original and outlier free data has been presented using boxplot and histograms. We simplified the character variables related to wind directions: WindDir9am, WindDir3pm, WindGustDir by changing data into dominant wind direction N, E, S and W. We extracted the month from date.

## Selecting the features

We created delta varibles for numerical variables: **WindChange**, **DeltaWindSpeed**, **DeltaHumidity**, **DeltaPressure**, **DeltaCloud**.

We filtrated into two groups one where RainTomorrow = 1 and the other where RainTomorrow = 0 in order to check the character data as in original python file. Looking at distributions of character variables we did not notice any differences and we used previousley chosen **RainToday**, **WindChange** and **WindGusDir**, because of the difference in plots between situations where RainTommorow is equal to 0 and 1.

Numerical variables has been chosen as in original Python model (based on their correlation with target variable). Variables with correlation above 0.09: **Sunshine**, **Pressure9am**, **Pressure3pm**, **Temp3pm**, **MaxTemp**, **Evaporation**, **WindGustSpeed**, **Rainfall**, **Cloud9am**, **Humidity9am**, **DeltaHumidity**, **Cloud3pm**, **Humidity3pm**, **DeltaPressure**.

We checked the correlation between the chosen numerical variables using the heatmap from its analysis we noticed that there is still a strong correlation between **Pressure9am** and **Pressure3pm** as well as between **MaxTemp** and **Temp3pm**, from which we dropped the Pressure9am and Temp3pm for consistency. We checked once again if there is no new high correlations between the variables.

## Balancing and Standardizing

Our character features were **Month**, **RainToday**, **WindChange**, **WindGustDir**, **RainTomorrow** using the SMOTENC. We created dummy variables for our nonbinary categorical variables (k-1 dummy variables for factors with k levels, this wasn't done in jupyter notebook code, but it should have been. Subsequently we standardized the features using MinMaxScaler in order to avoid any strong changes to the original distribution and checked it using mean() in a loop where mean values were not the same as in the jupyter notebook Python code, most probably due to the fact, that SMOTE_NC function works in slightly different way. We split the data into train - 70% and test - 30%.

## Training with different models

We recreated the four different models (Simple Logistic Regression, Support Vector Classifier, Random Forest Classifier and Extreme Gradient Boosting Classifier) in order to look for the possible changes in results. The final classification of the models stays the same aside from minor differences in results. We decided to exclude Tuned Logistic Regression as there was not much to be done.

```         
              The accuracy of the models compared with null accuracy:
```

Null accuracy - 77%\

Simple Logistic Regression RStudio - 80% vs Python - 80%\
Tuned Logistic Regression RStudio - Excluded vs Python - 80%\
Support Vector Classifier RStudio - 83% vs Python - 83%\
Random Forest Classifier RStudio - 90% vs Python - 84%\
Extreme Gradient Boosting Classifier RStudio - 91% vs Python - 90%\

```         
                           The recall of the models:
```

Simple Logistic Regression RStudio - 80% vs Python - 80%\
Tuned Logistic Regression RStudio - Excluded vs Python - 80%\
Support Vector Classifier RStudio - 82% vs Python - 85%\
Random Forest Classifier RStudio - 89% vs Python - 85%\
Extreme Gradient Boosting Classifier RStudio - 93% vs Python - 88%\

```         
                        The precission of the models:
```

Simple Logistic Regression RStudio - 80% vs Python - 80%\
Tuned Logistic Regression RStudio - Excluded vs Python - 80%\
Support Vector Classifier RStudio - 85% vs Python - 82%\
Random Forest Classifier RStudio - 91% vs Python - 83%\
Extreme Gradient Boosting Classifier RStudio - 89% vs Python - 92%\

```         
                   The area under the ROC curve of the models:
```

Simple Logistic Regression RStudio - 80% vs Python - 80%\
Tuned Logistic Regression RStudio - Excluded vs Python - 80%\
Support Vector Classifier RStudio - 84% vs Python - 83%\
Random Forest Classifier RStudio - 90% vs Python - 84%\
Extreme Gradient Boosting Classifier RStudio - 91% vs Python - 90%\
